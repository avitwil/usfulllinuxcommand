
---

## **Imports והגדרות ראשוניות**

```python
import os
import requests
from bs4 import BeautifulSoup
```

* `os` – מאפשר עבודה עם קבצים ותיקיות במערכת ההפעלה.
* `requests` – ספרייה לשליחת בקשות HTTP.
* `BeautifulSoup` – ספרייה לניתוח HTML ו־XML.

```python
URL_ATTRS = [
    "href", "src", "srcset", "poster",
    "action", "formaction",
    "content", "background", "cite", "longdesc",
    "manifest", "profile", "ping",
    "archive", "data", "codebase", "usemap",
    "data-url", "data-href", "data-link", "data-src"
]
```

* רשימה של תכונות HTML שמכילות כתובות URL.
* הקוד ינסה לחלץ את הערכים שלהם מכל תגית.

---

## **פונקציה `fetch_html`**

```python
def fetch_html(url: str, timeout: int = 10) -> tuple[str | None, str | None]:
    ...
```

* שולחת בקשה ל־`url` ומחזירה את ה־HTML והכתובת הסופית.
* `timeout` – מספר השניות להמתין לפני שהבקשה נכשלת.
* שימוש ב־`try/except` כדי למנוע קריסה אם הבקשה נכשלה.

```python
resp = requests.get(url, timeout=timeout)
resp.raise_for_status()
```

* שולח GET לכתובת.
* אם ה־HTTP לא הצליח (למשל 404), נזרקת חריגה.

```python
return resp.text, resp.url
```

* מחזיר את תוכן הדף ואת ה־URL הסופי (יכול להשתנות אם יש Redirect).

```python
except Exception as e:
    print(f"[fetch_html ERROR] {e}")
    return None, None
```

* אם משהו נכשל, מדפיס את השגיאה ומחזיר `None`.

---

## **פונקציה `normalize_url`**

```python
def normalize_url(base_url: str, link: str) -> str | None:
    ...
```

* הופך כתובת יחסית לכתובת מוחלטת (absolute URL).
* דואג להתעלם מ־hash, javascript, או ערכים ריקים.

```python
if not link or link.startswith("#") or link.lower().startswith("javascript:"):
    return None
```

* מתעלם מקישורים פנימיים או JavaScript.

```python
if link.startswith(("http://", "https://")):
    return link
```

* אם כבר כתובת מוחלטת – מחזיר כמו שהיא.

```python
if link.startswith("//"):
    return "https:" + link
```

* אם זה `//domain.com/...` – מוסיף `https:`.

```python
if link.startswith("/"):
    domain = base_url.split("//", 1)[-1].split("/", 1)[0]
    scheme = base_url.split("://")[0]
    return f"{scheme}://{domain}{link}"
```

* אם זה לינק יחסית לדומיין (`/path`) – בונה URL מלא.

```python
if base_url.endswith("/"):
    return base_url + link
return base_url.rsplit("/", 1)[0] + "/" + link
```

* אם זה לינק יחסית לדף – מחבר את ה־base\_url עם הקובץ.

* כל ה־`try/except` כדי למנוע קריסה במקרה של קלט לא תקין.

---

## **פונקציה `get_base_url`**

```python
def get_base_url(soup: BeautifulSoup, default_base: str) -> str:
```

* מחפשת תגית `<base href="...">` ב־HTML.
* אם קיימת – מחזירה אותה, אחרת מחזירה את כתובת ברירת המחדל.

---

## **פונקציה `extract_links_from_soup`**

```python
def extract_links_from_soup(soup: BeautifulSoup, base_url: str) -> list[str]:
```

* עוברת על כל תגית ב־HTML ומוציאה את כל ה־URLs מתכונות המופיעות ברשימה `URL_ATTRS`.

```python
for tag in soup.find_all(True):
    for attr in URL_ATTRS:
        if tag.has_attr(attr):
            value = tag[attr]
```

* לולאה על כל תגית וכל תכונה שמכילה URL.

```python
if attr == "srcset":
    for part in value.split(","):
        url = part.strip().split(" ")[0]
        full = normalize_url(base_url, url)
        if full:
            links.append(full)
```

* במיוחד ל־`srcset` שמכיל מספר כתובות, מפרידים לפי `,` ואז לוקחים את ה־URL הראשון.

```python
else:
    if isinstance(value, list):
        for v in value:
            full = normalize_url(base_url, v)
            if full:
                links.append(full)
    else:
        full = normalize_url(base_url, value)
        if full:
            links.append(full)
```

* טיפוסי ערכים אחרים – לבדוק אם רשימה או מחרוזת בודדת.

```python
return list(set(links))
```

* מחזיר רשימה של URLs ייחודיים.

---

## **פונקציה `extract_all_links_from_html`**

```python
def extract_all_links_from_html(html: str, base_url: str = "") -> list[str]:
```

* מקבלת HTML גולמי, משתמשת ב־BeautifulSoup לניתוח.
* מוציאה את ה־base\_url עם `get_base_url`.
* מחזירה את כל הקישורים עם `extract_links_from_soup`.

---

## **פונקציה `extract_all_links_from_url`**

```python
def extract_all_links_from_url(url: str, timeout: int = 10) -> list[str]:
```

* שולחת בקשה ל־URL, מקבלת HTML, ומוציאה את כל הקישורים.
* מחזירה רשימה של כתובות ייחודיות.

---

## **פונקציות להורדת קבצים**

```python
def create_downloads_folder():
    if not os.path.exists("downloads"):
        os.makedirs("downloads")
```

* בודקת אם תיקיית `downloads` קיימת, אם לא – יוצרת אותה.

```python
def download_if_file(url: str):
    if url.lower().endswith((".pdf", ".jpg", ".jpeg", ".png", ".gif")):
        file_name = os.path.join("downloads", url.split("/")[-1])
        print(f"Downloading: {file_name}")
        resp = requests.get(url)
        resp.raise_for_status()
        with open(file_name, "wb") as f:
            f.write(resp.content)
```

* מורידה את הקובץ אם זה סיומת נתמכת ושומרת בתיקיית `downloads`.

---

## **פונקציה `crawl_site`**

```python
def crawl_site(base_url: str, max_pages: int = 500) -> set[str]:
```

* סורקת אתר שלם מהכתובת הראשית.
* מוגבלת ל־`max_pages` דפים.
* שומרת לינקים שביקרנו בהם ומי שיש לבקר.
* מורידה קבצים תוך כדי סריקה.

```python
visited = set()
to_visit = set([base_url])
domain = base_url.split("//", 1)[-1].split("/", 1)[0]
all_links = set()
create_downloads_folder()
```

* אתחול מערכות: visited, to\_visit, domain, all\_links, יצירת תיקייה להורדה.

```python
while to_visit and len(visited) < max_pages:
    url = to_visit.pop()
    visited.add(url)
    print(f"Crawling: {url}")

    links = extract_all_links_from_url(url)
    all_links.update(links)
```

* לולאת סריקה: לוקחת URL, מסירה מה־to\_visit, מוסיפה ל־visited, ומוציאה את כל הלינקים מהדף.

```python
for link in links:
    if link.startswith("http://") or link.startswith("https://"):
        link_domain = link.split("//", 1)[-1].split("/", 1)[0]
        if link_domain == domain and link not in visited:
            to_visit.add(link)
    download_if_file(link)
```

* מוסיפה לינקים פנימיים ל־to\_visit.
* מורידה קבצים אם זה סיומת נתמכת.

---

## **חלק הרצה ראשי**

```python
if __name__ == "__main__":
    site_url = input("Insert site URL to crawl: ")
    found_links = crawl_site(site_url, max_pages=100)
    print(f"\nTotal unique links found: {len(found_links)}")
    print("\n".join(found_links))
```

* מבקש מהמשתמש URL.
* קורא ל־crawl\_site.
* מדפיס את מספר הלינקים שנמצאו ואת כולם.

---
